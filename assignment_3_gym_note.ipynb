{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Advancedn topics in ML - Assignment III - task A</u></h1>\n",
    "<h4>Omer Nivron</h4>\n",
    "15098443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#observation=[position of cart, velocity of cart, angle of pole, rotation rate of pole]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def observationToState(o, thresh):\n",
    "    s = int(0);\n",
    "    exp = 1;\n",
    "    for i in range(len(o)):\n",
    "        if thresh[i] == 0:\n",
    "            continue\n",
    "        ox = (min(thresh[i], max(-thresh[i], o[i])) + thresh[i]);\n",
    "        val = int((ox * float(steps)) / (2.0 * thresh[i]));\n",
    "        if val == steps:\n",
    "            val = steps - 1\n",
    "        s += val * exp;\n",
    "        exp *= steps;\n",
    "    return s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part A</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 3\n",
    "size=(3,5)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        print \n",
    "        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "        return_matrix[i]=np.append(j+1,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part B</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 100\n",
    "size=(100,5)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        print \n",
    "        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "        return_matrix[i]=np.append(j+1,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(np.mean(return_matrix,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.std(return_matrix,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part C</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "lr=[10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 0.5]\n",
    "lr=lr[2]\n",
    "batch_s=100\n",
    "y=.99\n",
    "n_eps=2000\n",
    "num_eps_test=10\n",
    "ran=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path=\"/Users/onivron/trained_models/ass_3/tensor_plot/task_A/part_C/part_c_lr_10_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Var creation</h3>\n",
    "We build data placeholders & variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('states'):\n",
    "    states=tf.placeholder(\"float32\",[None,4],name='states')\n",
    "with tf.name_scope('s_ne'):\n",
    "    s_ne=tf.placeholder(\"float32\",[None,4],name='s_ne')\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "    b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "with tf.name_scope('targetQ'):\n",
    "    targetQ=tf.placeholder(\"float32\",[None,2],name='targetQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def current_observation_to_action(states):\n",
    "    h1 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q=tf.matmul(h1,W2)+b2\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_observation_to_action(s_ne):\n",
    "    h2 = tf.nn.relu(tf.matmul(s_ne,W1)+ b1)\n",
    "    Q_prime=tf.stop_gradient(tf.matmul(h2,W2)+b2)\n",
    "    return Q_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss & optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "Q=current_observation_to_action(states)\n",
    "Q_prime=next_observation_to_action(s_ne)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss=tf.reduce_mean(tf.square((targetQ) -(Q)))\n",
    "    \n",
    "with tf.name_scope('train'):   \n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_offline_observ(num_episodes):\n",
    "    store=[]\n",
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        d = False\n",
    "        j = 0     \n",
    "        while j < 300:\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            if (d==True):\n",
    "                r=-1\n",
    "                #print(\"Episode finished after {} timesteps\".format(j))            \n",
    "            tmp_st=np.append(a,s)\n",
    "            tmp_st=np.append(tmp_st,s1)\n",
    "            tmp_st=np.append(tmp_st,r)\n",
    "            store=np.append(store,tmp_st)\n",
    "            if (d==True):\n",
    "                break\n",
    "            j+=1\n",
    "            s=s1\n",
    "    store=np.reshape(store,[-1,10])\n",
    "    return(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "observations=collect_offline_observ(n_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard summaries for loss & accuracy \n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "summary_op=tf.summary.merge_all()\n",
    "\n",
    "writer_opt=tf.summary.FileWriter(logs_path,graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "sess=tf.Session() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"------- Reloading previous parameters and continuing training...\")\n",
    "#folder='/Users/onivron/trained_models/ass_3/weights/lr'\n",
    "saver = tf.train.import_meta_graph(folder+'/accu.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint(folder+'/'))\n",
    "all_vars = tf.get_collection('vars')\n",
    "for v in all_vars:\n",
    "    sess.run(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "lo_List = []\n",
    "obser_len=np.shape(observations)[0]\n",
    "num_batch=int(obser_len/batch_s)\n",
    "#avg_eps_len=np.zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    for batch_num in range(num_batch):\n",
    "        \n",
    "        batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batch_s)\n",
    "        \n",
    "        batch_cur_st=np.reshape(observations[[batch_rows],1:5],[batch_s,4])\n",
    "        batch_nex_st=np.reshape(observations[[batch_rows],5:9],[batch_s,4])\n",
    "        \n",
    "        actions=np.reshape(observations[[batch_rows],0],[100,1])\n",
    "        \n",
    "        rewards=np.reshape(observations[[batch_rows+1],-1],[100,1])\n",
    "        delta=rewards+1\n",
    "        #delta=1\n",
    "        # I get for allQ a [1,2] vector \n",
    "        current_Q=sess.run(Q,feed_dict={states:batch_cur_st})\n",
    "        target_Q=np.copy(current_Q)\n",
    "        \n",
    "        Q_s_prime=sess.run(Q_prime,feed_dict={s_ne:batch_nex_st})\n",
    "        maxQ1=np.reshape(np.max(Q_s_prime,1),[100,1])\n",
    "        \n",
    "        row_idx = np.array(range(batch_s))\n",
    "        target_Q[row_idx[:,None],[actions]]=delta*y*maxQ1+rewards \n",
    "        _,lo,summary=sess.run([trainer,loss,summary_op],feed_dict={states:batch_cur_st,targetQ:target_Q})\n",
    "        lo_List.append(lo)\n",
    "        #if ((epoch%2==0) & (batch_num==100)):\n",
    "            #eps_len=np.zeros(100)\n",
    "        #    for eps in range(num_eps_test):\n",
    "        #        s=env.reset()\n",
    "        #        s=np.reshape(s,[1,4])\n",
    "        #        j=0\n",
    "        #        while j < 300:\n",
    "        #            j+=1\n",
    "        #            test_Q=sess.run(Q_prime,feed_dict={states:s})\n",
    "        #            act=np.argmax(test_Q)\n",
    "        #            s1,r,d,_ = env.step(act)\n",
    "        #            s1=np.reshape(s1,[1,4])\n",
    "        #            s=s1\n",
    "        #            if (d==True):\n",
    "                        #in_eps_len=np.copy(j)\n",
    "                        #eps_len[eps]=in_eps_len\n",
    "        #                break       \n",
    "        #    print(\"Episode finished after {} timesteps\".format(j))  \n",
    "            #avg_eps_len[epoch/2]=np.mean(eps_len)\n",
    "    print (lo)\n",
    "    writer_opt.add_summary(summary,(epoch))\n",
    "    folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/'+str(lr)\n",
    "    save_path=saver.save(sess,folder+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eps_len=np.zeros(100)\n",
    "avg_eps_len=np.zeros(epochs)\n",
    "for eps in range(100):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    s_=np.reshape(s,[1,4])\n",
    "    d = False\n",
    "    j = 0     \n",
    "    while j < 300:\n",
    "        a = sess.run(Q_prime,feed_dict={s_ne:s_})\n",
    "        ac=np.argmax(a)\n",
    "        s1,r,d,_ = env.step(ac)\n",
    "        r=0\n",
    "        if (d==True):\n",
    "            r=-1\n",
    "            print(\"Episode finished after {} timesteps\".format(j))\n",
    "            in_eps_len=np.copy(j)\n",
    "            eps_len[eps]=in_eps_len\n",
    "            break\n",
    "        j+=1\n",
    "        s_=np.reshape(s1,[1,4])\n",
    "avg_eps_len=np.mean(eps_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_eps_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part D</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr=[10^-5, 10^-4, 10^-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "#lr = 1\n",
    "y = .99\n",
    "num_episodes=2000\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize network with random small weights\n",
    "Observe starting state s\n",
    "Repeat:\n",
    "Choose a from s using policy derived from Q (e.g. greedy) Take action a, observe r, s′\n",
    "Update Q by backpropagating the error signal given by E = Q(s, a) − r + γ maxa Q(s′, a)\n",
    "s ← s′"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states=tf.placeholder(shape=[1,4],dtype=tf.float32) \n",
    "with tf.name_scope(\"weights\"):\n",
    "    W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "    b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def observation_to_action(states):\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime=tf.stop_gradient(tf.matmul(h2,W2)+b2)\n",
    "    predict=tf.argmax(Q,1)\n",
    "    return predict,Q,Q_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "pred,Q,Q_prime=observation_to_action(states)\n",
    "loss = tf.reduce_sum(tf.square((nextQ) - Q)/2)\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "run_avg_loss=[]\n",
    "run_avg_eps_len=[]\n",
    "size=(2000,100)\n",
    "jList = np.zeros(size)\n",
    "lo_List = np.zeros(size)\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "for run in range(1):\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            Q1 = sess.run(Q_prime,feed_dict={states:s1})            \n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ=np.copy(allQ)\n",
    "            if d == False:\n",
    "                r=0\n",
    "                targetQ[0,a]=y*(maxQ1)+r\n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})\n",
    "            else:\n",
    "                r=-1\n",
    "                targetQ[0,a]=r \n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})\n",
    "                if (i%50==0):\n",
    "                    print(\"Episode finished after {} timesteps with loss {}\".format(j+1,lo))            \n",
    "                break\n",
    "            s = s1                  \n",
    "        lo_List[:,run]=(lo)\n",
    "        jList[:,run]=(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#state_dim   = env.observation_space.shape[0]\n",
    "#num_actions = env.action_space.n\n",
    "#Initialize table with all zeros\n",
    "#Q = np.zeros([env.observation_space.shape[0],env.action_space.n])\n",
    "#    for i in range(num_episodes):\n",
    "#    #Reset environment and get first new observation\n",
    "#        s = env.reset()\n",
    "#        print(s)\n",
    "#        rAll = 0\n",
    "#        d = False\n",
    "#        j = 0\n",
    "#        #The Q-Table learning algorithm\n",
    "#        while j < 99:\n",
    "#            j+=1\n",
    "#            #Choose an action by greedily (with noise) picking from Q table\n",
    "#            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    " ##           #Get new state and reward from environment\n",
    "#           s1,r,d,_ = env.step(a)\n",
    "#            r=0\n",
    "            #Update Q-Table with new knowledge\n",
    "#            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "#            s = s1\n",
    "#            if d == True:\n",
    "#                r=-1\n",
    "#                rAll += r\n",
    " #               print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "#               break\n",
    "#        rList.append(rAll)\n",
    "#        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    " #       print \"Final Q-Table Values\"\n",
    "#       print Q"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
