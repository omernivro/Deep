{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Advancedn topics in ML - Assignment III - task A</u></h1>\n",
    "<h4>Omer Nivron</h4>\n",
    "15098443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "from pylab import plot, ylim, xlim, show, xlabel, ylabel, grid\n",
    "import matplotlib\n",
    "from __future__ import division\n",
    "matplotlib.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_eps_len(List,epochs,title,lr_d):\n",
    "    line={}\n",
    "    colors=['b','k','g','c','y','r']\n",
    "    x1=range(epochs)\n",
    "    for i in range(np.shape(List)[1]):\n",
    "        y = List[:,i]\n",
    "        line[i]=plt.plot(x1,y,colors[i],label=lr_d[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Eps length')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=3,prop={'size':9})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(List,epochs,title,lr_d):\n",
    "    line={}\n",
    "    colors=['b','k','g','c','y','r']\n",
    "    x1=range(epochs)\n",
    "    for i in range(np.shape(List)[1]):\n",
    "        y = List[:,i]\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        line[i]=plt.plot(x1,y,colors[i],label=lr_d[i])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Bellman loss')\n",
    "        plt.title(title)\n",
    "        plt.legend(loc=3,prop={'size':7})\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-11 18:59:07,082] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env._max_episode_steps = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part A</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []\n",
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 3\n",
    "size=(3,2)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                return_=-1*y**j\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        return_matrix[i]=np.append(j+1,return_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(return_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part B</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []\n",
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 100\n",
    "size=(100,2)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                return_=-1*y**j\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        return_matrix[i]=np.append(j+1,return_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(np.mean(return_matrix,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.std(return_matrix,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part C linear layer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#epochs=100\n",
    "lr_d=[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "#batch_s=100\n",
    "y=.99\n",
    "#n_eps=2000\n",
    "num_eps_test=100\n",
    "st=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    with tf.name_scope('states'):\n",
    "        states=tf.placeholder(\"float32\",[None,4],name='states')\n",
    "    with tf.name_scope('s_ne'):\n",
    "        s_ne=tf.placeholder(\"float32\",[None,4],name='s_ne')\n",
    "    with tf.name_scope('targetQ'):\n",
    "        targetQ=tf.placeholder(\"float32\",[None,2],name='targetQ')\n",
    "    return(states,s_ne,targetQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(lr,states,s_ne,targetQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    Q = tf.matmul(states,W1)+ b1\n",
    "    Q_prime =tf.matmul(s_ne,W1)+ b1\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))\n",
    "    # don't divide the squared term by 2, since Q and next Q are of size (None,2)-see main loop explanation below\n",
    "    # where half of the terms are equal \n",
    "    # meaning yielding 0\n",
    "    with tf.name_scope('loss'):\n",
    "        loss=tf.reduce_mean(tf.square((targetQ) -(Q)))\n",
    "    with tf.name_scope('train'):   \n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_epoch(sess):\n",
    "    eps_len=[]\n",
    "    avg_eps_len=[]\n",
    "    return_=[]\n",
    "    for eps in range(num_eps_test):\n",
    "        s=env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            test_Q=sess.run(Q,feed_dict={states:s})\n",
    "            act=np.argmax(test_Q)\n",
    "            s1,r,d,_ = env.step(act)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j+1)\n",
    "                tmp_ret=-1*y**j\n",
    "                return_.append(tmp_ret)\n",
    "                break       \n",
    "    avg_eps_len.append(np.mean(eps_len))\n",
    "    return (avg_eps_len,np.mean(return_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Model for learning rate: ', 1e-05)\n",
      "('Mean episode length:', [170.43000000000001])\n",
      "('Mean return:', -0.19597523321795379)\n",
      "('Model for learning rate: ', 0.0001)\n",
      "('Mean episode length:', [160.05000000000001])\n",
      "('Mean return:', -0.20901814531939614)\n",
      "('Model for learning rate: ', 0.001)\n",
      "('Mean episode length:', [12.73])\n",
      "('Mean return:', -0.89004216968569272)\n",
      "('Model for learning rate: ', 0.01)\n",
      "('Mean episode length:', [10.33])\n",
      "('Mean return:', -0.91051278153113668)\n",
      "('Model for learning rate: ', 0.1)\n",
      "('Mean episode length:', [120.76000000000001])\n",
      "('Mean return:', -0.31850727886681762)\n",
      "('Model for learning rate: ', 0.5)\n",
      "('Mean episode length:', [10.31])\n",
      "('Mean return:', -0.91070285771074166)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "for lr in lr_d:\n",
    "    save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/linear/'+str(lr)+'/'\n",
    "    save_model = os.path.join(save_MDir,'accu')\n",
    "    states,s_ne,targetQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,s_ne,targetQ)\n",
    "    # restore model and calculate mean length and return over n_test episodes\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess = sess, save_path= save_model)\n",
    "        print(\"Model for learning rate: \",lr)\n",
    "        leng,ret = test_epoch(sess)\n",
    "        print('Mean episode length:',leng)\n",
    "        print('Mean return:',ret)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_offline_observ(num_episodes):\n",
    "    store=[]\n",
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        d = False\n",
    "        j = 0     \n",
    "        while j < 300:\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            if (d==True):\n",
    "                r=-1\n",
    "                #print(\"Episode finished after {} timesteps\".format(j))            \n",
    "            tmp_st=np.append(a,s)\n",
    "            tmp_st=np.append(tmp_st,s1)\n",
    "            tmp_st=np.append(tmp_st,r)\n",
    "            store=np.append(store,tmp_st)\n",
    "            if (d==True):\n",
    "                break\n",
    "            j+=1\n",
    "            s=s1\n",
    "    store=np.reshape(store,[-1,10])\n",
    "    return(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations=collect_offline_observ(n_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "size=(100,6)\n",
    "all_loss=np.zeros(size)\n",
    "all_len=np.zeros(size)\n",
    "for lr in lr_d:\n",
    "    tf.reset_default_graph()\n",
    "    states,s_ne,targetQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,s_ne,targetQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    lo_List = []\n",
    "    eps_len=[]\n",
    "    obser_len=np.shape(observations)[0]\n",
    "    num_batch=int(obser_len/batch_s)\n",
    "    #avg_eps_len=np.zeros(epochs)\n",
    "    for epoch in range(epochs):\n",
    "        length,ret=test_epoch(sess,epoch)\n",
    "        eps_len.append(length)\n",
    "\n",
    "        for batch_num in range(num_batch):\n",
    "\n",
    "            batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batch_s)\n",
    "\n",
    "            batch_cur_st=np.reshape(observations[[batch_rows],1:5],[batch_s,4])\n",
    "\n",
    "            batch_nex_st=np.reshape(observations[[batch_rows],5:9],[batch_s,4])\n",
    "\n",
    "            actions=np.reshape(observations[[batch_rows],0],[100,1])\n",
    "\n",
    "            rewards=np.reshape(observations[[batch_rows+1],-1],[100,1])\n",
    "            delta=rewards+1\n",
    "            #delta=1\n",
    "            # I get for allQ a [1,2] vector \n",
    "            current_Q,max_Q1=sess.run([Q,maxQ1],feed_dict={states:batch_cur_st,s_ne:batch_nex_st})\n",
    "            target_Q=np.copy(current_Q)\n",
    "            max_Q1=np.reshape(max_Q1,[100,1])     \n",
    "            row_idx = np.array(range(batch_s))\n",
    "            target_Q[row_idx[:,None],[actions]]=delta*y*max_Q1+rewards \n",
    "            _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,s_ne:batch_nex_st,targetQ:target_Q})\n",
    "        lo_List.append(lo)\n",
    "        # save weights & graph when epoch finished\n",
    "        folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/linear/'+str(lr)\n",
    "        save_path=saver.save(sess,folder+'/accu')\n",
    "    # save length and loss per lr in different (st) column\n",
    "    all_loss[:,st]=lo_List\n",
    "    all_len[:,st]=np.reshape(np.asarray(eps_len),[100])\n",
    "    st+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_len,epochs,'Avg episode length',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss(all_loss[:,0:3],epochs,'Avg loss',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss(all_loss[:,3:6],epochs,'Avg loss',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part C NN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#epochs=100\n",
    "lr_d=[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "#batch_s=100\n",
    "y=.99\n",
    "#n_eps=2000\n",
    "num_eps_test=100\n",
    "#st=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Var creation</h3>\n",
    "We build data placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    with tf.name_scope('states'):\n",
    "        states=tf.placeholder(\"float32\",[None,4],name='states')\n",
    "    with tf.name_scope('s_ne'):\n",
    "        s_ne=tf.placeholder(\"float32\",[None,4],name='s_ne')\n",
    "    with tf.name_scope('targetQ'):\n",
    "        targetQ=tf.placeholder(\"float32\",[None,2],name='targetQ')\n",
    "    return(states,s_ne,targetQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(lr,states,s_ne,targetQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(s_ne,W1)+ b1)\n",
    "    Q=tf.matmul(h1,W2)+b2\n",
    "    Q_prime=tf.matmul(h2,W2)+b2\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))\n",
    "    # don't divide the squared term by 2, since Q and next Q are of size (None,2)-see main loop explanation below\n",
    "    # where half of the terms are equal \n",
    "    # meaning yielding 0\n",
    "    with tf.name_scope('loss'):\n",
    "        loss=tf.reduce_mean(tf.square((targetQ) -(Q)))\n",
    "    with tf.name_scope('train'):   \n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def test_epoch(sess):\n",
    "    eps_len=[]\n",
    "    avg_eps_len=[]\n",
    "    return_=[]\n",
    "    for eps in range(num_eps_test):\n",
    "        s=env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            test_Q=sess.run(Q_prime,feed_dict={s_ne:s})\n",
    "            act=np.argmax(test_Q)\n",
    "            s1,r,d,_ = env.step(act)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j+1)\n",
    "                tmp_ret=-1*y**j\n",
    "                return_.append(tmp_ret)\n",
    "                break       \n",
    "    avg_eps_len.append(np.mean(eps_len))\n",
    "    return (avg_eps_len,np.mean(return_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Model for learning rate: ', 1e-05)\n",
      "('Mean episode length:', [198.56999999999999])\n",
      "('Mean return:', -0.15878467088198508)\n",
      "('Model for learning rate: ', 0.0001)\n",
      "('Mean episode length:', [254.13])\n",
      "('Mean return:', -0.085906518153689049)\n",
      "('Model for learning rate: ', 0.001)\n",
      "('Mean episode length:', [111.54000000000001])\n",
      "('Mean return:', -0.33164593666098968)\n",
      "('Model for learning rate: ', 0.01)\n",
      "('Mean episode length:', [153.81])\n",
      "('Mean return:', -0.22754721346806381)\n",
      "('Model for learning rate: ', 0.1)\n",
      "('Mean episode length:', [244.19999999999999])\n",
      "('Mean return:', -0.099073264242567696)\n",
      "('Model for learning rate: ', 0.5)\n",
      "('Mean episode length:', [198.80000000000001])\n",
      "('Mean return:', -0.15703093967886414)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "for lr in lr_d:\n",
    "    save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/'+str(lr)+'/'\n",
    "    save_model = os.path.join(save_MDir,'accu')\n",
    "    states,s_ne,targetQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,s_ne,targetQ)\n",
    "    # restore model and calculate mean length and return over n_test episodes\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess = sess, save_path= save_model)\n",
    "        print(\"Model for learning rate: \",lr)\n",
    "        leng,ret = test_epoch(sess)\n",
    "        print('Mean episode length:',leng)\n",
    "        print('Mean return:',ret)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_offline_observ(num_episodes):\n",
    "    store=[]\n",
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        d = False\n",
    "        j = 0     \n",
    "        while j < 300:\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            if (d==True):\n",
    "                r=-1\n",
    "                #print(\"Episode finished after {} timesteps\".format(j))            \n",
    "            tmp_st=np.append(a,s)\n",
    "            tmp_st=np.append(tmp_st,s1)\n",
    "            tmp_st=np.append(tmp_st,r)\n",
    "            store=np.append(store,tmp_st)\n",
    "            if (d==True):\n",
    "                break\n",
    "            j+=1\n",
    "            s=s1\n",
    "    store=np.reshape(store,[-1,10])\n",
    "    return(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "observations=collect_offline_observ(n_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "size=(100,6)\n",
    "all_loss=np.zeros(size)\n",
    "all_len=np.zeros(size)\n",
    "for lr in lr_d:\n",
    "    tf.reset_default_graph()\n",
    "    states,s_ne,targetQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,s_ne,targetQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    lo_List = []\n",
    "    eps_len=[]\n",
    "    obser_len=np.shape(observations)[0]\n",
    "    num_batch=int(obser_len/batch_s)\n",
    "    #avg_eps_len=np.zeros(epochs)\n",
    "    for epoch in range(epochs):\n",
    "        length,ret=test_epoch(sess)\n",
    "        eps_len.append(length)\n",
    "\n",
    "        for batch_num in range(num_batch):\n",
    "\n",
    "            batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batch_s)\n",
    "\n",
    "            batch_cur_st=np.reshape(observations[[batch_rows],1:5],[batch_s,4])\n",
    "\n",
    "            batch_nex_st=np.reshape(observations[[batch_rows],5:9],[batch_s,4])\n",
    "\n",
    "            actions=np.reshape(observations[[batch_rows],0],[100,1])\n",
    "\n",
    "            rewards=np.reshape(observations[[batch_rows+1],-1],[100,1])\n",
    "            delta=rewards+1\n",
    "            #delta=1\n",
    "            # I get for allQ a [1,2] vector \n",
    "            current_Q,max_Q1=sess.run([Q,maxQ1],feed_dict={states:batch_cur_st,s_ne:batch_nex_st})\n",
    "            target_Q=np.copy(current_Q)\n",
    "            max_Q1=np.reshape(max_Q1,[100,1])     \n",
    "            row_idx = np.array(range(batch_s))\n",
    "            target_Q[row_idx[:,None],[actions]]=delta*y*max_Q1+rewards \n",
    "            _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,targetQ:target_Q})\n",
    "        lo_List.append(lo)\n",
    "        # save weights & graph when epoch finished\n",
    "        folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/'+str(lr)\n",
    "        save_path=saver.save(sess,folder+'/accu')\n",
    "    # save length and loss per lr in different (st) column\n",
    "    all_loss[:,st]=lo_List\n",
    "    all_len[:,st]=np.reshape(np.asarray(eps_len),[100])\n",
    "    st+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_len,epochs,'Avg episode length',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(all_loss[:,0:3],epochs,'Avg loss',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss(all_loss[:,3:6],epochs,'Avg loss',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part D</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "lr_d=[0.0001]\n",
    "y = .99\n",
    "#num_episodes=2000\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    states=tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "    nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "    return(states,nextQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(lr,states,nextQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime=(tf.matmul(h2,W2)+b2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))\n",
    "    # don't divide the squared term by 2, since Q and next Q are of size (None,2)-see main loop explanation below\n",
    "    # where half of the terms are equal \n",
    "    # meaning yielding 0\n",
    "    loss = tf.reduce_sum(tf.square((nextQ) - Q))\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return (Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Avg episode length:', [20.600000000000001])\n",
      "('Avg return:', -0.81372275713299391)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_d/'+str(lr_d[0])+'/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "states,targetQ=plh()\n",
    "Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,targetQ)\n",
    "eps_len=[]\n",
    "return_=[]\n",
    "avg_eps_len=[]\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess = sess, save_path= save_model)\n",
    "# 100 testing episodes   \n",
    "    for i in range(100):\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j)\n",
    "                tmp_ret=-1*y**j\n",
    "                return_.append(tmp_ret)\n",
    "                break\n",
    "    avg_eps_len.append(np.mean(eps_len))    \n",
    "    print('Avg episode length:',avg_eps_len)\n",
    "    print('Avg return:',np.mean(return_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "run_avg_loss=[]\n",
    "run_avg_eps_len=[]\n",
    "size=(2000,100)\n",
    "jList = np.zeros(size)\n",
    "lo_List = np.zeros(size)\n",
    "\n",
    "for run in range(100):\n",
    "    tf.reset_default_graph()\n",
    "    states,nextQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,nextQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            max_Q1 = sess.run(maxQ1,feed_dict={states:s1})            \n",
    "            targetQ=np.copy(allQ)\n",
    "            if d == False:\n",
    "                r=0\n",
    "                targetQ[0,a]=y*(max_Q1)+r\n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})\n",
    "            else:\n",
    "                r=-1\n",
    "                targetQ[0,a]=r \n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})          \n",
    "                break\n",
    "            s = s1\n",
    "        lo_List[i,run]=(lo)\n",
    "        jList[i,run]=(j+1)\n",
    "# save weights & graph when episodes finished\n",
    "folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_d/'+str(lr_d[0])\n",
    "save_path=saver.save(sess,folder+'/accu')\n",
    "# avg over 100 columns (distinct runs) to get avg loss & eps length \n",
    "all_loss=np.reshape(np.mean(lo_List,1),[2000,1])\n",
    "all_leng=np.reshape(np.mean(jList,1),[2000,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_leng,2000,'Avg episode length over 100 runs, each of 2000 episodes',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_loss,2000,'Avg loss over 100 runs, each of 2000 episodes',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part E - 30 hidden units</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=.99\n",
    "#num_episodes=2000\n",
    "lr=[0.0001]\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "    states=tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "    return(states,nextQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(states,nextQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 30],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2= tf.get_variable(\"W2\", [30, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [30],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime=(tf.matmul(h2,W2)+b2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))\n",
    "    # don't divide the squared term by 2, since Q and next Q are of size (None,2)-see main loop explanation below\n",
    "    # where half of the terms are equal \n",
    "    # meaning yielding 0\n",
    "    loss = tf.reduce_sum(tf.square((nextQ) - Q))\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Avg episode length:', [28.780000000000001])\n",
      "('Avg return:', -0.74944155348373387)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_e/'+str(30)+'/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "states,nextQ=plh()\n",
    "Q,maxQ1,Q_prime,loss,trainer=graph(states,nextQ)\n",
    "eps_len=[]\n",
    "avg_eps_len=[]\n",
    "return_=[]\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess = sess, save_path= save_model)\n",
    "# 100 testing episodes   \n",
    "    for i in range(100):\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j)\n",
    "                tmp_ret=-1*y**j\n",
    "                return_.append(tmp_ret)\n",
    "                break\n",
    "    avg_eps_len.append(np.mean(eps_len))    \n",
    "    print('Avg episode length:',avg_eps_len)\n",
    "    print('Avg return:',np.mean(return_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network with 30 hidden units\n",
    "size=(101,1)\n",
    "jList_30 = np.zeros(size)\n",
    "lo_List_30 = np.zeros(size)\n",
    "for run in range(1):\n",
    "    tf.reset_default_graph()\n",
    "    states,nextQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(states,nextQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init=tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            max_Q1 = sess.run(maxQ1,feed_dict={states:s1})            \n",
    "            targetQ=np.copy(allQ)\n",
    "            if d == False:\n",
    "                r=0\n",
    "                targetQ[0,a]=y*(max_Q1)+r\n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})\n",
    "            else:\n",
    "                r=-1\n",
    "                targetQ[0,a]=r \n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})          \n",
    "                break\n",
    "            s = s1\n",
    "        if ((i+1)%20==0):\n",
    "            lo_List_30[(i+1)/20,run]=(lo)\n",
    "            jList_30[(i+1)/20,run]=(j)\n",
    "    # save weights & graph when episode finished\n",
    "    folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_e/'+str(30)\n",
    "    save_path=saver.save(sess,folder+'/accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot eps len\n",
    "x1=np.arange(0,2020,20)\n",
    "line=plt.plot(x1,jList_30,'m',label='0.0001')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Eps length')\n",
    "plt.title('Episode length over 2000 epochs - \\n 30 hidden units model')\n",
    "plt.legend(loc=3,prop={'size':9})    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot loss\n",
    "x1=np.arange(0,2020,20)\n",
    "line=plt.plot(x1,lo_List_30,'c',label='0.0001')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Bellman loss')\n",
    "plt.title('Loss over 2000 epochs - \\n 30 hidden units model')\n",
    "plt.legend(loc=3,prop={'size':9})    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part E - 1000 hidden units</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=.99\n",
    "#num_episodes=2000\n",
    "lr=[0.0001]\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    states_net_2=tf.placeholder(shape=[1,4],dtype=tf.float32) \n",
    "    nextQ_2 = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "    return(states_net_2,nextQ_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(states_net_2,nextQ_2):\n",
    "    with tf.name_scope(\"weights_net_2\"):\n",
    "        W1_ne_2 = tf.get_variable(\"W1_ne_2\", [4, 1000],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2_ne_2 = tf.get_variable(\"W2_ne_2\", [1000, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases_net_2\"):\n",
    "        b1_ne_2 = tf.get_variable(\"b1_ne_2\", [1000],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2_ne_2 = tf.get_variable(\"b2_ne_2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states_net_2, W1_ne_2) + b1_ne_2)\n",
    "    Q_net_2=tf.matmul(h1, W2_ne_2) + b2_ne_2\n",
    "    h2 = tf.nn.relu(tf.matmul(states_net_2,W1_ne_2)+ b1_ne_2)\n",
    "    Q_prime_net_2=(tf.matmul(h2,W2_ne_2)+b2_ne_2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime_net_2,1))\n",
    "    # don't divide the squared term by 2, since Q and next Q are of size (None,2)-see main loop explanation below\n",
    "    # where half of the terms are equal \n",
    "    # meaning yielding 0\n",
    "    loss = tf.reduce_sum(tf.square((nextQ_2) - Q_net_2))\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return(Q_net_2,maxQ1,Q_prime_net_2,loss,trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Avg episode length:', [300.0])\n",
      "('Avg return:', -0.049040894071285736)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_e/'+str(1000)+'/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "states_net_2,nextQ_2=plh()\n",
    "Q_net_2,maxQ1,Q_prime_net_2,loss,trainer=graph(states_net_2,nextQ_2)\n",
    "eps_len=[]\n",
    "avg_eps_len=[]\n",
    "return_=[]\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess = sess, save_path= save_model)\n",
    "# 100 testing episodes   \n",
    "    for i in range(100):\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            allQ = sess.run(Q_net_2,feed_dict={states_net_2:s})\n",
    "            a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j)\n",
    "                tmp_ret=-1*y**j\n",
    "                return_.append(tmp_ret)\n",
    "                break\n",
    "    avg_eps_len.append(np.mean(eps_len))    \n",
    "    print('Avg episode length:',avg_eps_len)\n",
    "    print('Avg return:',np.mean(return_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network with 1000 hidden units\n",
    "size=(101,1)\n",
    "jList_1000 = np.zeros(size)\n",
    "lo_List_1000 = np.zeros(size)\n",
    "for run in range(1):\n",
    "    tf.reset_default_graph()\n",
    "    states_net_2,nextQ_2=plh()\n",
    "    Q_net_2,maxQ1,Q_prime_net_2,loss,trainer=graph(states_net_2,nextQ_2)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q_net_2,feed_dict={states_net_2:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            max_Q1 = sess.run(maxQ1,feed_dict={states_net_2:s1})            \n",
    "            targetQ=np.copy(allQ)\n",
    "            if d == False:\n",
    "                r=0\n",
    "                targetQ[0,a]=y*(max_Q1)+r\n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states_net_2:s,nextQ_2:targetQ})\n",
    "            else:\n",
    "                r=-1\n",
    "                targetQ[0,a]=r \n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states_net_2:s,nextQ_2:targetQ})          \n",
    "                break\n",
    "            s = s1\n",
    "        if ((i+1)%20==0):\n",
    "            lo_List_1000[(i+1)/20,run]=(lo)\n",
    "            jList_1000[(i+1)/20,run]=(j)\n",
    "    # save weights & graph when episode finished\n",
    "    folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_e/'+str(1000)\n",
    "    save_path=saver.save(sess,folder+'/accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot eps len\n",
    "x1=np.arange(0,2020,20)\n",
    "line=plt.plot(x1,jList_1000,'m',label='0.0001')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Eps length')\n",
    "plt.title('Episode length over 2000 epochs - \\n 1000 hidden units model')\n",
    "plt.legend(loc=3,prop={'size':9})    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot loss\n",
    "x1=np.arange(0,2020,20)\n",
    "line=plt.plot(x1,lo_List_1000,'c',label='0.0001')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Bellman loss')\n",
    "plt.title('Loss over 2000 epochs - \\n 1000 hidden units model')\n",
    "plt.legend(loc=3,prop={'size':9})    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part F</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = .99\n",
    "#num_episodes=2000\n",
    "num_eps_test=100\n",
    "e=0.05\n",
    "lr_d=[0.0001]\n",
    "#batchSize = 32\n",
    "#bufferx=100\n",
    "#h=0\n",
    "#lo=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    states=tf.placeholder(shape=[None,4],dtype=tf.float32) \n",
    "    nextQ = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
    "    return(states,nextQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(states,nextQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime=(tf.matmul(h2,W2)+b2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))\n",
    "    # don't divide the squared term by 2, since Q and next Q are of size (None,2)-see main loop explanation below\n",
    "    # where half of the terms are equal \n",
    "    # meaning yielding 0\n",
    "    loss = tf.reduce_mean(tf.square((nextQ) - Q))\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_epoch(sess):\n",
    "    eps_len=[]\n",
    "    for eps in range(num_eps_test):\n",
    "        st_te=env.reset()\n",
    "        st_te=np.reshape(st_te,[1,4])\n",
    "        t=0\n",
    "        while t < 300:\n",
    "            t+=1\n",
    "            test_Q=sess.run(Q,feed_dict={states:st_te})\n",
    "            act=np.argmax(test_Q)\n",
    "            new_st_te,re,done,_ = env.step(act)\n",
    "            new_st_te=np.reshape(new_st_te,[1,4])\n",
    "            st_te=new_st_te\n",
    "            if (done==True):\n",
    "                eps_len.append(t)\n",
    "                tmp_rt=-1*y**t\n",
    "                return_.append(tmp_rt)\n",
    "                break                \n",
    "    return(np.mean(eps_len),np.mean(return_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Model for learning rate: ', 0.0001)\n",
      "('Mean episode length:', 230.90000000000001)\n",
      "('Mean return:', -0.080722965344674269)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_f/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "states,nextQ=plh()\n",
    "Q,maxQ1,Q_prime,loss,trainer=graph(states,nextQ)\n",
    "# restore model and calculate mean length and return over n_test episodes\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess = sess, save_path= save_model)\n",
    "    print(\"Model for learning rate: \",lr_d[0])\n",
    "    leng,ret = test_epoch(sess)\n",
    "    print('Mean episode length:',leng)\n",
    "    print('Mean return:',ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "replay = []\n",
    "lo_List = []\n",
    "eps_le=[]\n",
    "tf.reset_default_graph()\n",
    "states,nextQ=plh()\n",
    "Q,maxQ1,Q_prime,loss,trainer=graph(states,nextQ)\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "sess=tf.Session() \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for epoch in range(num_episodes):\n",
    "    if epoch%20==0:\n",
    "        length,ret=test_epoch(sess)\n",
    "        eps_le.append(length)\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    s=np.reshape(s,[1,4])\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Network\n",
    "    while j < 300:\n",
    "        j+=1\n",
    "        allQ = sess.run(Q,feed_dict={states:s})\n",
    "        if np.random.rand(1) < e:\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "        else:\n",
    "            a=np.argmax(allQ)\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        if (d==True):\n",
    "            r=-1    \n",
    "        else:\n",
    "            r=0\n",
    "        s1=np.reshape(s1,[1,4])\n",
    "        #Experience replay storage\n",
    "        if (len(replay) < bufferx): #if buffer not filled, add to it\n",
    "            replay.append((s, a, r, s1))\n",
    "        else: #if buffer full, overwrite old values\n",
    "            if (h < (bufferx-1)):\n",
    "                h += 1\n",
    "            else:\n",
    "                h = 0\n",
    "            replay[h] = (s, a, r, s1)        \n",
    "            observations=np.copy(replay)\n",
    "            #randomly sample our experience replay memory\n",
    "            batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batchSize)\n",
    "            batch_cur_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],0])),[batchSize,4])\n",
    "            batch_nex_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],3])),[batchSize,4])\n",
    "            actions=np.reshape(np.concatenate(observations[[batch_rows],1]),[batchSize,1])\n",
    "            rewards=np.reshape(np.concatenate(observations[[batch_rows+1],2]),[batchSize,1])\n",
    "            delta=rewards+1\n",
    "            current_Q=sess.run(Q,feed_dict={states:batch_cur_st})\n",
    "            target_Q=np.copy(current_Q)\n",
    "            max_Q1=sess.run(maxQ1,feed_dict={states:batch_nex_st})\n",
    "            max_Q1=np.reshape(max_Q1,[batchSize,1])\n",
    "            row_idx = np.array(range(batchSize))\n",
    "            target_Q[row_idx[:,None],[actions]]=delta*y*max_Q1+rewards \n",
    "            _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,nextQ:target_Q})\n",
    "        if (d==True):\n",
    "            break\n",
    "        s=s1\n",
    "    if ((epoch%20==0) & (lo!=0)):\n",
    "        lo_List.append(lo)\n",
    "# save weights & graph when episodes finished\n",
    "folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_f/'\n",
    "save_path=saver.save(sess,folder+'/accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_eps_len(np.reshape(eps_le,[100,1]),100,'Episode length over 2000 episodes reporting every 20 steps \\n - 100 units W/ replay buffer',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(np.reshape(lo_List,[99,1]),99,'Loss over 2000 episodes reporting every 20 steps \\n - 100 units W/ replay buffer',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part G</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=.99\n",
    "#num_episodes=2000\n",
    "num_eps_test=100\n",
    "e=0.05\n",
    "lr_d=[0.0001]\n",
    "#batchSize = 32\n",
    "#bufferx=100\n",
    "#h=0\n",
    "#lo=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    states=tf.placeholder(shape=[None,4],dtype=tf.float32) \n",
    "    nextQ = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
    "    s_ne=tf.placeholder(shape=[None,4],dtype=tf.float32) \n",
    "    return(states,nextQ,s_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def graph(states,nextQ):\n",
    "    hidden_layer = {'W': tf.Variable(tf.random_uniform([4, 100], 0, 0.01)),\n",
    "                    'b': tf.Variable(tf.random_uniform([100], 0, 0.01))}\n",
    "\n",
    "    output_layer = {'W': tf.Variable(tf.random_uniform([100, 2], 0, 0.01)),\n",
    "                    'b': tf.Variable(tf.random_uniform([2], 0, 0.01))}   \n",
    "    h1= tf.nn.relu(tf.matmul(states,hidden_layer['W'])+ hidden_layer['b'])\n",
    "    Q=tf.matmul(h1,output_layer['W'])+output_layer['b']\n",
    "    loss = tf.reduce_mean(tf.square((nextQ) - Q))\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return(Q,trainer,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_target(s_ne):\n",
    "    hidden_layer_t = {'W': tf.Variable(tf.random_uniform([4, 100], 0, 0.01)),\n",
    "                    'b': tf.Variable(tf.random_uniform([100], 0, 0.01))}\n",
    "\n",
    "    output_layer_t = {'W': tf.Variable(tf.random_uniform([100, 2], 0, 0.01)),\n",
    "                    'b': tf.Variable(tf.random_uniform([2], 0, 0.01))}\n",
    "    h2_n = tf.nn.relu(tf.matmul(s_ne,hidden_layer_t['W'])+ hidden_layer_t['b'])\n",
    "    Q_prime_n=tf.matmul(h2_n,output_layer_t['W'])+output_layer_t['b']\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime_n,1))\n",
    "    return(maxQ1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_target_net(tf_vars, sess):\n",
    "# Build list of copy operations\n",
    "    num_vars = int(len(tf_vars)/2)  # 4 in each net (2 weights, 2 biases)\n",
    "    op_list = []\n",
    "    for idx, var in enumerate(tf_vars[0:num_vars]):\n",
    "        op_list.append(tf_vars[idx+num_vars].assign(var.value()))\n",
    "    # Run the TF operations to copy variables\n",
    "    for op in op_list:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_epoch(sess):\n",
    "    eps_len=[]\n",
    "    return_=[]\n",
    "    for eps in range(num_eps_test):\n",
    "        st_te=env.reset()\n",
    "        st_te=np.reshape(st_te,[1,4])\n",
    "        t=0\n",
    "        while t < 300:\n",
    "            t+=1\n",
    "            test_Q=sess.run(Q,feed_dict={states:st_te})\n",
    "            act=np.argmax(test_Q)\n",
    "            new_st_te,re,done,_ = env.step(act)\n",
    "            new_st_te=np.reshape(new_st_te,[1,4])\n",
    "            st_te=new_st_te\n",
    "            if (done==True):\n",
    "                eps_len.append(t)\n",
    "                tmp_rt=-1*y**t\n",
    "                return_.append(tmp_rt)\n",
    "                break                \n",
    "    return(np.mean(eps_len),np.mean(return_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "all_loss=[]\n",
    "replay = []\n",
    "eps_le=[]\n",
    "tf.reset_default_graph()\n",
    "states,nextQ,s_ne=plh()\n",
    "Q,trainer,loss=graph(states,nextQ)\n",
    "maxQ1=graph_target(s_ne)\n",
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "sess=tf.Session() \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for epoch in range(num_episodes):\n",
    "    if (epoch%5==0):\n",
    "        train_vars = tf.trainable_variables() \n",
    "        update_target_net(train_vars,sess)\n",
    "    if epoch%20==0:\n",
    "        length,ret=test_epoch(sess)\n",
    "        eps_le.append(length)\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    s=np.reshape(s,[1,4])\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Network\n",
    "    while j < 300:\n",
    "        #env.render()\n",
    "        j+=1\n",
    "        allQ = sess.run(Q,feed_dict={states:s})\n",
    "        if np.random.rand(1) < e:\n",
    "            a=np.sum(np.random.uniform(0,1)>0.5)\n",
    "        else:\n",
    "            a=np.argmax(allQ)\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        if (d==True):\n",
    "            r=-1    \n",
    "        else:\n",
    "            r=0\n",
    "        s1=np.reshape(s1,[1,4])\n",
    "        #Experience replay storage\n",
    "        if (len(replay) < bufferx): #if buffer not filled, add to it\n",
    "            replay.append((s, a, r, s1))\n",
    "        else: #if buffer full, overwrite old values\n",
    "            if (h < (bufferx-1)):\n",
    "                h += 1\n",
    "            else:\n",
    "                h = 0\n",
    "            replay[h] = (s, a, r, s1)        \n",
    "            observations=np.copy(replay)\n",
    "            #randomly sample our experience replay memory\n",
    "            batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batchSize)\n",
    "            batch_cur_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],0])),[batchSize,4])\n",
    "            batch_nex_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],3])),[batchSize,4])\n",
    "            actions=np.reshape(np.concatenate(observations[[batch_rows],1]),[batchSize,1])\n",
    "            rewards=np.reshape(np.concatenate(observations[[batch_rows+1],2]),[batchSize,1])\n",
    "            delta=rewards+1\n",
    "            current_Q=sess.run(Q,feed_dict={states:batch_cur_st})\n",
    "            target_Q=np.copy(current_Q)\n",
    "            max_Q1=sess.run(maxQ1,feed_dict={s_ne:batch_nex_st})\n",
    "            max_Q1=np.reshape(max_Q1,[batchSize,1])\n",
    "            row_idx = np.array(range(batchSize))\n",
    "            target_Q[row_idx[:,None],[actions]]=delta*y*max_Q1+rewards \n",
    "            _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,nextQ:target_Q})\n",
    "        if (d==True):\n",
    "            break\n",
    "        s=s1\n",
    "    if ((epoch%20==0) & (lo!=0)):\n",
    "        all_loss.append(lo)\n",
    "# save weights & graph when episodes finished\n",
    "folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_g/'\n",
    "save_path=saver.save(sess,folder+'/accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(np.reshape(eps_le,[100,1]),100,'Episode length over 2000 episodes reporting every 20 steps \\n - 100 units W/ replay buffer',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(np.reshape(all_loss,[99,1]),99,'Episode length over 2000 episodes reporting every 20 steps \\n - 100 units W/ replay buffer',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Model for learning rate: ', 0.0001)\n",
      "('Mean episode length:', 183.02000000000001)\n",
      "('Mean return:', -0.15981916176067379)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_g/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "states,nextQ,s_ne=plh()\n",
    "Q,trainer,loss=graph(states,nextQ)\n",
    "maxQ1=graph_target(s_ne)\n",
    "# restore model and calculate mean length and return over n_test episodes\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess = sess, save_path= save_model)\n",
    "    print(\"Model for learning rate: \",lr_d[0])\n",
    "    leng,ret = test_epoch(sess)\n",
    "    print('Mean episode length:',leng)\n",
    "    print('Mean return:',ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part H</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
