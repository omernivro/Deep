{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Advancedn topics in ML - Assignment III - task A</u></h1>\n",
    "<h4>Omer Nivron</h4>\n",
    "15098443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#observation=[position of cart, velocity of cart, angle of pole, rotation rate of pole]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def observationToState(o, thresh):\n",
    "    s = int(0);\n",
    "    exp = 1;\n",
    "    for i in range(len(o)):\n",
    "        if thresh[i] == 0:\n",
    "            continue\n",
    "        ox = (min(thresh[i], max(-thresh[i], o[i])) + thresh[i]);\n",
    "        val = int((ox * float(steps)) / (2.0 * thresh[i]));\n",
    "        if val == steps:\n",
    "            val = steps - 1\n",
    "        s += val * exp;\n",
    "        exp *= steps;\n",
    "    return s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.step(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part A</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 3\n",
    "size=(3,5)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        print \n",
    "        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "        return_matrix[i]=np.append(j+1,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part B</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 100\n",
    "size=(100,5)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        print \n",
    "        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "        return_matrix[i]=np.append(j+1,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(np.mean(return_matrix,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.std(return_matrix,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part C</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-24 20:01:35,676] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "lr=[10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 0.5]\n",
    "lr=lr[2]\n",
    "batch_s=100\n",
    "y=.99\n",
    "n_eps=2000\n",
    "num_eps_test=10\n",
    "ran=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs_path=\"/Users/onivron/trained_models/ass_3/tensor_plot/task_A/part_C/part_c_lr_10_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Var creation</h3>\n",
    "We build data placeholders & variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('states'):\n",
    "    states=tf.placeholder(\"float32\",[None,4],name='states')\n",
    "with tf.name_scope('s_ne'):\n",
    "    s_ne=tf.placeholder(\"float32\",[None,4],name='s_ne')\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "    b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "with tf.name_scope('targetQ'):\n",
    "    targetQ=tf.placeholder(\"float32\",[None,2],name='targetQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def current_observation_to_action(states):\n",
    "    h1 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q=tf.matmul(h1,W2)+b2\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_observation_to_action(s_ne):\n",
    "    h1 = tf.nn.relu(tf.matmul(s_ne,W1)+ b1)\n",
    "    Q_prime=tf.stop_gradient(tf.matmul(h1,W2)+b2)\n",
    "    return Q_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss & optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "Q=current_observation_to_action(states)\n",
    "Q_prime=next_observation_to_action(s_ne)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss=tf.reduce_mean(tf.square((targetQ) -(Q)))\n",
    "    \n",
    "with tf.name_scope('train'):   \n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_offline_observ(num_episodes):\n",
    "    store=[]\n",
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        d = False\n",
    "        j = 0     \n",
    "        while j < 300:\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            if (d==True):\n",
    "                r=-1\n",
    "                #print(\"Episode finished after {} timesteps\".format(j))            \n",
    "            tmp_st=np.append(a,s)\n",
    "            tmp_st=np.append(tmp_st,s1)\n",
    "            tmp_st=np.append(tmp_st,r)\n",
    "            store=np.append(store,tmp_st)\n",
    "            if (d==True):\n",
    "                break\n",
    "            j+=1\n",
    "            s=s1\n",
    "    store=np.reshape(store,[-1,10])\n",
    "    return(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "observations=collect_offline_observ(n_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard summaries for loss & accuracy \n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "summary_op=tf.summary.merge_all()\n",
    "\n",
    "writer_opt=tf.summary.FileWriter(logs_path,graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "sess=tf.Session() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"------- Reloading previous parameters and continuing training...\")\n",
    "#folder='/Users/onivron/trained_models/ass_3/weights/lr'\n",
    "saver = tf.train.import_meta_graph(folder+'/accu.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint(folder+'/'))\n",
    "all_vars = tf.get_collection('vars')\n",
    "for v in all_vars:\n",
    "    sess.run(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:27: VisibleDeprecationWarning: non integer (and non boolean) array-likes will not be accepted as indices in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00484276\n",
      "0.00192754\n",
      "0.00472981\n",
      "0.00559159\n",
      "0.00461164\n",
      "0.00272932\n",
      "0.00536262\n",
      "0.00266067\n",
      "0.00435515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b5eac77147ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbatch_cur_st\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_rows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_nex_st\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_rows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "lo_List = []\n",
    "obser_len=np.shape(observations)[0]\n",
    "num_batch=int(obser_len/batch_s)\n",
    "#avg_eps_len=np.zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    for batch_num in range(num_batch):\n",
    "        \n",
    "        batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batch_s)\n",
    "        \n",
    "        batch_cur_st=np.reshape(observations[[batch_rows],1:5],[batch_s,4])\n",
    "        batch_nex_st=np.reshape(observations[[batch_rows],5:9],[batch_s,4])\n",
    "        \n",
    "        actions=np.reshape(observations[[batch_rows],0],[100,1])\n",
    "        \n",
    "        rewards=np.reshape(observations[[batch_rows+1],-1],[100,1])\n",
    "        delta=rewards+1\n",
    "        #delta=1\n",
    "        # I get for allQ a [1,2] vector \n",
    "        current_Q=sess.run(Q,feed_dict={states:batch_cur_st})\n",
    "        target_Q=np.copy(current_Q)\n",
    "        \n",
    "        Q_s_prime=sess.run(Q_prime,feed_dict={s_ne:batch_nex_st})\n",
    "        maxQ1=np.reshape(np.max(Q_s_prime,1),[100,1])\n",
    "        \n",
    "        row_idx = np.array(range(batch_s))\n",
    "        target_Q[row_idx[:,None],[actions]]=delta*y*maxQ1+rewards \n",
    "        _,lo,summary=sess.run([trainer,loss,summary_op],feed_dict={states:batch_cur_st,targetQ:target_Q})\n",
    "        lo_List.append(lo)\n",
    "        #if ((epoch%2==0) & (batch_num==100)):\n",
    "            #eps_len=np.zeros(100)\n",
    "        #    for eps in range(num_eps_test):\n",
    "        #        s=env.reset()\n",
    "        #        s=np.reshape(s,[1,4])\n",
    "        #        j=0\n",
    "        #        while j < 300:\n",
    "        #            j+=1\n",
    "        #            test_Q=sess.run(Q_prime,feed_dict={states:s})\n",
    "        #            act=np.argmax(test_Q)\n",
    "        #            s1,r,d,_ = env.step(act)\n",
    "        #            s1=np.reshape(s1,[1,4])\n",
    "        #            s=s1\n",
    "        #            if (d==True):\n",
    "                        #in_eps_len=np.copy(j)\n",
    "                        #eps_len[eps]=in_eps_len\n",
    "        #                break       \n",
    "        #    print(\"Episode finished after {} timesteps\".format(j))  \n",
    "            #avg_eps_len[epoch/2]=np.mean(eps_len)\n",
    "    print (lo)\n",
    "    writer_opt.add_summary(summary,(epoch))\n",
    "    folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/'+str(lr)\n",
    "    save_path=saver.save(sess,folder+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 7 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n"
     ]
    }
   ],
   "source": [
    "eps_len=np.zeros(100)\n",
    "avg_eps_len=np.zeros(epochs)\n",
    "for eps in range(100):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    s_=np.reshape(s,[1,4])\n",
    "    d = False\n",
    "    j = 0     \n",
    "    while j < 300:\n",
    "        a = sess.run(Q_prime,feed_dict={s_ne:s_})\n",
    "        ac=np.argmax(a)\n",
    "        s1,r,d,_ = env.step(ac)\n",
    "        r=0\n",
    "        if (d==True):\n",
    "            r=-1\n",
    "            print(\"Episode finished after {} timesteps\".format(j))\n",
    "            in_eps_len=np.copy(j)\n",
    "            eps_len[eps]=in_eps_len\n",
    "            break\n",
    "        j+=1\n",
    "        s_=np.reshape(s1,[1,4])\n",
    "avg_eps_len=np.mean(eps_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_eps_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part D</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr=[10^-5, 10^-4, 10^-3, 10^-2, 10^-1, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "#Q = np.zeros([env.observation_space.shape[0],env.action_space.n])\n",
    "# Set learning parameters\n",
    "#lr = 1\n",
    "#y = .99\n",
    "#num_episodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "#rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    for i in range(num_episodes):\n",
    "#    #Reset environment and get first new observation\n",
    "#        s = env.reset()\n",
    "#        print(s)\n",
    "#        rAll = 0\n",
    "#        d = False\n",
    "#        j = 0\n",
    "#        #The Q-Table learning algorithm\n",
    "#        while j < 99:\n",
    "#            j+=1\n",
    "#            #Choose an action by greedily (with noise) picking from Q table\n",
    "#            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    " ##           #Get new state and reward from environment\n",
    "#           s1,r,d,_ = env.step(a)\n",
    "#            r=0\n",
    "            #Update Q-Table with new knowledge\n",
    "#            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "#            s = s1\n",
    "#            if d == True:\n",
    "#                r=-1\n",
    "#                rAll += r\n",
    " #               print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "#               break\n",
    "#        rList.append(rAll)\n",
    "#        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    " #       print \"Final Q-Table Values\"\n",
    "#       print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize network with random small weights\n",
    "Observe starting state s\n",
    "Repeat:\n",
    "Choose a from s using policy derived from Q (e.g. greedy) Take action a, observe r, s′\n",
    "Update Q by backpropagating the error signal given by E = Q(s, a) − r + γ maxa Q(s′, a)\n",
    "s ← s′"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_dim   = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "batch_s=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states=tf.placeholder(shape=[batch_s,4],dtype=tf.float32) \n",
    "W1 = tf.get_variable(\"W1\", [4, 100],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "b1 = tf.get_variable(\"b1\", [100],\n",
    "                       initializer=tf.constant_initializer(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer())\n",
    "b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0))\n",
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def observation_to_action(states):\n",
    "  # define policy neural network\n",
    "\n",
    "  h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "\n",
    "  Q=tf.matmul(h1, W2) + b2\n",
    "  predict=tf.argmax(Q,1)\n",
    "  return predict,Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "pred,Q=observation_to_action(states)\n",
    "loss = tf.reduce_sum(tf.square(tf.stop_gradient(nextQ) - Q))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=10**(-5))\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = .99\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    s=np.reshape(s,[1,4])\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Network\n",
    "    while j < 300:\n",
    "        #env.render()\n",
    "        j+=1\n",
    "        a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "        # I get for allQ a [1,2] vector \n",
    "        allQ = sess.run([Q],feed_dict={states:s})\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        s1=np.reshape(s1,[1,4])\n",
    "        r=0\n",
    "        Q1 = sess.run(Q,feed_dict={states:s1})            \n",
    "        maxQ1 = np.max(Q1)\n",
    "        targetQ = allQ\n",
    "        (targetQ)[0][0,a] =(y*(maxQ1)+r)      \n",
    "        targetQ=np.reshape(targetQ,[1,2])\n",
    "        _,lo=sess.run([updateModel,loss],feed_dict={states:s1,nextQ:targetQ})\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            r=-1\n",
    "            rAll += r\n",
    "            if (i%100==0):\n",
    "                print(\"Episode finished after {} timesteps with loss {}\".format(j+1,lo))            \n",
    "            break\n",
    "    jList.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "state_dim   = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "def observation_to_action(states):\n",
    "  # define policy neural network\n",
    "  W1 = tf.get_variable(\"W1\", [state_dim, 100],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "  b1 = tf.get_variable(\"b1\", [100],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "  W2 = tf.get_variable(\"W2\", [100, num_actions],\n",
    "                       initializer=tf.random_normal_initializer())\n",
    "  b2 = tf.get_variable(\"b2\", [num_actions],\n",
    "                       initializer=tf.constant_initializer(0))\n",
    "  Q = tf.matmul(h1, W2) + b2\n",
    "  predict=tf.argmax(q,1)\n",
    "  return predict,Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Q))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Set learning parameters\n",
    "y = .99\n",
    "num_episodes = 1\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            allQ = sess.run([Q],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Q,feed_dict={inputs1:s1})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a] = r + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:s1,nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "10**(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_s=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(int(600000/batch_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "34%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_eps_len=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_eps_len+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_eps_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
