{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Advancedn topics in ML - Assignment III - task A</u></h1>\n",
    "<h4>Omer Nivron</h4>\n",
    "15098443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #line4,=plt.plot(x1, y1_av,\"b\",label='moving average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_eps_len(List,epochs,title,lr_d):\n",
    "    line={}\n",
    "    colors=['b','k','g','c','y','r']\n",
    "    x1=range(epochs)\n",
    "    for i in range(np.shape(List)[1]):\n",
    "        y = List[:,i]\n",
    "        line[i]=plt.plot(x1,y,colors[i],label=lr_d[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Eps length')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=3,prop={'size':9})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(List,epochs,title,lr_d):\n",
    "    line={}\n",
    "    colors=['b','k','g','c','y','r']\n",
    "    x1=range(epochs)\n",
    "    for i in range(np.shape(List)[1]):\n",
    "        y = List[:,i]\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        line[i]=plt.plot(x1,y,colors[i],label=lr_d[i])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Bellman loss')\n",
    "        plt.title(title)\n",
    "        plt.legend(loc=3,prop={'size':7})\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#observation=[position of cart, velocity of cart, angle of pole, rotation rate of pole]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part A</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []\n",
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 3\n",
    "size=(3,2)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                return_=-1*y**j\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        print \n",
    "        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "        return_matrix[i]=np.append(j+1,return_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(return_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part B</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards\n",
    "rList = []\n",
    "lr = 1\n",
    "y = .99\n",
    "num_episodes = 100\n",
    "size=(100,2)\n",
    "return_matrix=np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        print(s)\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                r=-1\n",
    "                rAll += r\n",
    "                return_=-1*y**j\n",
    "                print(\"Episode finished after {} timesteps\".format(j+1))\n",
    "                break\n",
    "        rList.append(rAll)\n",
    "        print \n",
    "        print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "        return_matrix[i]=np.append(j+1,return_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(np.mean(return_matrix,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.std(return_matrix,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part C linear layer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "from pylab import plot, ylim, xlim, show, xlabel, ylabel, grid\n",
    "import matplotlib\n",
    "from __future__ import division\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env._max_episode_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=100\n",
    "lr_d=[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "batch_s=100\n",
    "y=.99\n",
    "n_eps=2000\n",
    "num_eps_test=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    with tf.name_scope('states'):\n",
    "        states=tf.placeholder(\"float32\",[None,4],name='states')\n",
    "    with tf.name_scope('s_ne'):\n",
    "        s_ne=tf.placeholder(\"float32\",[None,4],name='s_ne')\n",
    "    with tf.name_scope('targetQ'):\n",
    "        targetQ=tf.placeholder(\"float32\",[None,2],name='targetQ')\n",
    "    return(states,s_ne,targetQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(lr,states,s_ne,targetQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    Q = (tf.matmul(states,W1)+ b1)\n",
    "    Q_prime =(tf.matmul(s_ne,W1)+ b1)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1) )  \n",
    "    with tf.name_scope('loss'):\n",
    "        loss=tf.reduce_mean(tf.square((targetQ) -(Q)))\n",
    "    with tf.name_scope('train'):   \n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_epoch(sess,epoch):\n",
    "    eps_len=[]\n",
    "    avg_eps_len=[]\n",
    "    for eps in range(num_eps_test):\n",
    "        s=env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            test_Q=sess.run(Q_prime,feed_dict={s_ne:s})\n",
    "            act=np.argmax(test_Q)\n",
    "            s1,r,d,_ = env.step(act)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j+1)\n",
    "                break       \n",
    "    avg_eps_len.append(np.mean(eps_len))\n",
    "    return (avg_eps_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_offline_observ(num_episodes):\n",
    "    store=[]\n",
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        d = False\n",
    "        j = 0     \n",
    "        while j < 300:\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            if (d==True):\n",
    "                r=-1\n",
    "                #print(\"Episode finished after {} timesteps\".format(j))            \n",
    "            tmp_st=np.append(a,s)\n",
    "            tmp_st=np.append(tmp_st,s1)\n",
    "            tmp_st=np.append(tmp_st,r)\n",
    "            store=np.append(store,tmp_st)\n",
    "            if (d==True):\n",
    "                break\n",
    "            j+=1\n",
    "            s=s1\n",
    "    store=np.reshape(store,[-1,10])\n",
    "    return(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations=collect_offline_observ(n_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "size=(100,6)\n",
    "all_loss=np.zeros(size)\n",
    "all_len=np.zeros(size)\n",
    "st=0\n",
    "for lr in lr_d:\n",
    "    tf.reset_default_graph()\n",
    "    states,s_ne,targetQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,s_ne,targetQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    lo_List = []\n",
    "    eps_len=[]\n",
    "    obser_len=np.shape(observations)[0]\n",
    "    num_batch=int(obser_len/batch_s)\n",
    "    #avg_eps_len=np.zeros(epochs)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        eps_len.append(test_epoch(sess,epoch))\n",
    "\n",
    "        for batch_num in range(num_batch):\n",
    "\n",
    "            batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batch_s)\n",
    "\n",
    "            batch_cur_st=np.reshape(observations[[batch_rows],1:5],[batch_s,4])\n",
    "\n",
    "            batch_nex_st=np.reshape(observations[[batch_rows],5:9],[batch_s,4])\n",
    "\n",
    "            actions=np.reshape(observations[[batch_rows],0],[100,1])\n",
    "\n",
    "            rewards=np.reshape(observations[[batch_rows+1],-1],[100,1])\n",
    "            delta=rewards+1\n",
    "            #delta=1\n",
    "            # I get for allQ a [1,2] vector \n",
    "            current_Q,max_Q1=sess.run([Q,maxQ1],feed_dict={states:batch_cur_st,s_ne:batch_nex_st})\n",
    "            target_Q=np.copy(current_Q)\n",
    "            max_Q1=np.reshape(max_Q1,[100,1])     \n",
    "            row_idx = np.array(range(batch_s))\n",
    "            target_Q[row_idx[:,None],[actions]]=delta*y*max_Q1+rewards \n",
    "            _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,s_ne:batch_nex_st,targetQ:target_Q})\n",
    "        lo_List.append(lo)\n",
    "        folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/linear/'+str(lr)\n",
    "        save_path=saver.save(sess,folder+'/accu')\n",
    "    all_loss[:,st]=lo_List\n",
    "    all_len[:,st]=eps_len\n",
    "    st+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_len,epochs,'a',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_loss(all_loss,epochs,'a',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part C NN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "from pylab import plot, ylim, xlim, show, xlabel, ylabel, grid\n",
    "import matplotlib\n",
    "from __future__ import division\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env._max_episode_steps = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs=100\n",
    "lr_d=[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "batch_s=100\n",
    "y=.99\n",
    "n_eps=2000\n",
    "num_eps_test=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Var creation</h3>\n",
    "We build data placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    with tf.name_scope('states'):\n",
    "        states=tf.placeholder(\"float32\",[None,4],name='states')\n",
    "    with tf.name_scope('s_ne'):\n",
    "        s_ne=tf.placeholder(\"float32\",[None,4],name='s_ne')\n",
    "    with tf.name_scope('targetQ'):\n",
    "        targetQ=tf.placeholder(\"float32\",[None,2],name='targetQ')\n",
    "    return(states,s_ne,targetQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(lr,states,s_ne,targetQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(s_ne,W1)+ b1)\n",
    "    Q=tf.matmul(h1,W2)+b2\n",
    "    Q_prime=tf.matmul(h2,W2)+b2\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1) )  \n",
    "    with tf.name_scope('loss'):\n",
    "        loss=tf.reduce_mean(tf.square((targetQ) -(Q)))\n",
    "    with tf.name_scope('train'):   \n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define loss & optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " def test_epoch(sess):\n",
    "    eps_len=[]\n",
    "    avg_eps_len=[]\n",
    "    for eps in range(num_eps_test):\n",
    "        s=env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            test_Q=sess.run(Q_prime,feed_dict={s_ne:s})\n",
    "            act=np.argmax(test_Q)\n",
    "            s1,r,d,_ = env.step(act)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j+1)\n",
    "                break       \n",
    "    avg_eps_len.append(np.mean(eps_len))\n",
    "    return (avg_eps_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_offline_observ(num_episodes):\n",
    "    store=[]\n",
    "    for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        d = False\n",
    "        j = 0     \n",
    "        while j < 300:\n",
    "            a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            r=0\n",
    "            if (d==True):\n",
    "                r=-1\n",
    "                #print(\"Episode finished after {} timesteps\".format(j))            \n",
    "            tmp_st=np.append(a,s)\n",
    "            tmp_st=np.append(tmp_st,s1)\n",
    "            tmp_st=np.append(tmp_st,r)\n",
    "            store=np.append(store,tmp_st)\n",
    "            if (d==True):\n",
    "                break\n",
    "            j+=1\n",
    "            s=s1\n",
    "    store=np.reshape(store,[-1,10])\n",
    "    return(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "observations=collect_offline_observ(n_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "size=(100,6)\n",
    "all_loss=np.zeros(size)\n",
    "all_len=np.zeros(size)\n",
    "st=0\n",
    "for lr in lr_d:\n",
    "    tf.reset_default_graph()\n",
    "    states,s_ne,targetQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,s_ne,targetQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    lo_List = []\n",
    "    eps_len=[]\n",
    "    obser_len=np.shape(observations)[0]\n",
    "    num_batch=int(obser_len/batch_s)\n",
    "    #avg_eps_len=np.zeros(epochs)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        eps_len.append(test_epoch(sess))\n",
    "\n",
    "        for batch_num in range(num_batch):\n",
    "\n",
    "            batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batch_s)\n",
    "\n",
    "            batch_cur_st=np.reshape(observations[[batch_rows],1:5],[batch_s,4])\n",
    "\n",
    "            batch_nex_st=np.reshape(observations[[batch_rows],5:9],[batch_s,4])\n",
    "\n",
    "            actions=np.reshape(observations[[batch_rows],0],[100,1])\n",
    "\n",
    "            rewards=np.reshape(observations[[batch_rows+1],-1],[100,1])\n",
    "            delta=rewards+1\n",
    "            #delta=1\n",
    "            # I get for allQ a [1,2] vector \n",
    "            current_Q,max_Q1=sess.run([Q,maxQ1],feed_dict={states:batch_cur_st,s_ne:batch_nex_st})\n",
    "            target_Q=np.copy(current_Q)\n",
    "            max_Q1=np.reshape(max_Q1,[100,1])     \n",
    "            row_idx = np.array(range(batch_s))\n",
    "            target_Q[row_idx[:,None],[actions]]=delta*y*max_Q1+rewards \n",
    "            _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,targetQ:target_Q})\n",
    "        lo_List.append(lo)\n",
    "        folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/'+str(lr)\n",
    "        save_path=saver.save(sess,folder+'/accu')\n",
    "    all_loss[:,st]=lo_List\n",
    "    all_len[:,st]=np.reshape(np.asarray(eps_len),[100])\n",
    "    st+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_len,epochs,'Avg episode length',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(all_loss[:,0:3],epochs,'Avg loss',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_loss(all_loss[:,3:6],epochs,'Avg loss',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "for lr in lr_d:\n",
    "    save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_c/'+str(lr)+'/'\n",
    "    save_model = os.path.join(save_MDir,'accu')\n",
    "    states,s_ne,targetQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,s_ne,targetQ)\n",
    "    # restore model and calculate mean length and return over n_test episodes\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess = sess, save_path= save_model)\n",
    "        print(\"Model for learning rate: \",lr)\n",
    "        leng = test_epoch(sess)\n",
    "        print('Mean episode length:',leng)\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part D</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "from pylab import plot, ylim, xlim, show, xlabel, ylabel, grid\n",
    "import matplotlib\n",
    "from __future__ import division\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env._max_episode_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "lr_d=[0.0001]\n",
    "y = .99\n",
    "num_episodes=2000\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    states=tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "    nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "    return(states,nextQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(lr,states,nextQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime=(tf.matmul(h2,W2)+b2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))  \n",
    "    loss = tf.reduce_sum(tf.square((nextQ) - Q)/2)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return (Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "run_avg_loss=[]\n",
    "run_avg_eps_len=[]\n",
    "size=(2000,100)\n",
    "jList = np.zeros(size)\n",
    "lo_List = np.zeros(size)\n",
    "\n",
    "for run in range(2):\n",
    "    tf.reset_default_graph()\n",
    "    states,nextQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,nextQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            max_Q1 = sess.run(maxQ1,feed_dict={states:s1})            \n",
    "            targetQ=np.copy(allQ)\n",
    "            if d == False:\n",
    "                r=0\n",
    "                targetQ[0,a]=y*(max_Q1)+r\n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})\n",
    "            else:\n",
    "                r=-1\n",
    "                targetQ[0,a]=r \n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})          \n",
    "                break\n",
    "            s = s1\n",
    "        lo_List[i,run]=(lo)\n",
    "        jList[i,run]=(j+1)\n",
    "folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_d/'+str(lr)\n",
    "save_path=saver.save(sess,folder+'/accu')\n",
    "all_loss=np.reshape(np.mean(lo_List,1),[2000,1])\n",
    "all_leng=np.reshape(np.mean(jList,1),[2000,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_loss=np.reshape(np.mean(lo_List,1),[2000,1])\n",
    "all_leng=np.reshape(np.mean(jList,1),[2000,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_leng,2000,'Avg episode length over 100 runs, each of 2000 episodes',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_eps_len(all_loss,2000,'Avg loss over 100 runs, each of 2000 episodes',lr_d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> testing w/ saved weights </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "save_MDir='/Users/onivron/trained_models/ass_3/weights/task_A_part_d/'+str(lr)+'/'\n",
    "save_model = os.path.join(save_MDir,'accu')\n",
    "states,targetQ=plh()\n",
    "Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,targetQ)\n",
    "eps_len=[]\n",
    "avg_eps_len=[]\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess = sess, save_path= save_model)\n",
    "# 100 testing episodes   \n",
    "    for i in range(100):\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        j=0\n",
    "        while j < 300:\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            s=s1\n",
    "            if (d==True):\n",
    "                eps_len.append(j)\n",
    "                break\n",
    "    avg_eps_len.append(np.mean(eps_len))    \n",
    "    print('Avg episode length:',avg_eps_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part E - 30 hidden units</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "from pylab import plot, ylim, xlim, show, xlabel, ylabel, grid\n",
    "import matplotlib\n",
    "from __future__ import division\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-10 23:45:32,351] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env._max_episode_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=.99\n",
    "num_episodes=2000\n",
    "lr=[0.0001]\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "    states=tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "    return(states,nextQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(states,nextQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 30],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2= tf.get_variable(\"W2\", [30, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [30],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime=(tf.matmul(h2,W2)+b2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))  \n",
    "    loss = tf.reduce_sum(tf.square((nextQ) - Q)/2)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:45: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/onivron/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:46: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# Network with 30 hidden units\n",
    "size=(101,1)\n",
    "jList_30 = np.zeros(size)\n",
    "lo_List_30 = np.zeros(size)\n",
    "for run in range(1):\n",
    "    tf.reset_default_graph()\n",
    "    states,nextQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(states,nextQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init=tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            max_Q1 = sess.run(maxQ1,feed_dict={states:s1})            \n",
    "            targetQ=np.copy(allQ)\n",
    "            if d == False:\n",
    "                r=0\n",
    "                targetQ[0,a]=y*(max_Q1)+r\n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})\n",
    "            else:\n",
    "                r=-1\n",
    "                targetQ[0,a]=r \n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})          \n",
    "                break\n",
    "            s = s1\n",
    "        if ((i+1)%20==0):\n",
    "            lo_List_30[(i+1)/20,run]=(lo)\n",
    "            jList_30[(i+1)/20,run]=(j)\n",
    "    folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_e/'+str(30)\n",
    "    save_path=saver.save(sess,folder+'/accu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(lo_List_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(jList_30[1:100,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part E - 1000 hidden units</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "from pylab import plot, ylim, xlim, show, xlabel, ylabel, grid\n",
    "import matplotlib\n",
    "from __future__ import division\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env._max_episode_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=.99\n",
    "num_episodes=2000\n",
    "lr=[0.0001]\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    states_net_2=tf.placeholder(shape=[1,4],dtype=tf.float32) \n",
    "    nextQ_2 = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "    return(states_net_2,nextQ_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(states_net_2,nextQ_2):\n",
    "    with tf.name_scope(\"weights_net_2\"):\n",
    "        W1_ne_2 = tf.get_variable(\"W1_ne_2\", [4, 1000],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2_ne_2 = tf.get_variable(\"W2_ne_2\", [1000, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases_net_2\"):\n",
    "        b1_ne_2 = tf.get_variable(\"b1_ne_2\", [1000],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2_ne_2 = tf.get_variable(\"b2_ne_2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states_net_2, W1_ne_2) + b1_ne_2)\n",
    "    Q_net_2=tf.matmul(h1, W2_ne_2) + b2_ne_2\n",
    "    h2 = tf.nn.relu(tf.matmul(states_net_2,W1_ne_2)+ b1_ne_2)\n",
    "    Q_prime_net_2=(tf.matmul(h2,W2_ne_2)+b2_ne_2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime_net,1))  \n",
    "    loss = tf.reduce_sum(tf.square((nextQ) - Q)/2)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network with 30 hidden units\n",
    "size=(101,1)\n",
    "jList_1000 = np.zeros(size)\n",
    "lo_List_1000 = np.zeros(size)\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "for run in range(1):\n",
    "    tf.reset_default_graph()\n",
    "    states,nextQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,nextQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            max_Q1 = sess.run(maxQ1,feed_dict={states:s1})            \n",
    "            targetQ=np.copy(allQ)\n",
    "            if d == False:\n",
    "                r=0\n",
    "                targetQ[0,a]=y*(max_Q1)+r\n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})\n",
    "            else:\n",
    "                r=-1\n",
    "                targetQ[0,a]=r \n",
    "                targetQ=np.reshape(targetQ,[1,2])\n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:s,nextQ:targetQ})          \n",
    "                break\n",
    "            s = s1\n",
    "        if ((i+1)%20==0):\n",
    "            lo_List_1000[(i+1),run]=(lo)\n",
    "            jList_1000[(i+1),run]=(j)\n",
    "    folder='/Users/onivron/trained_models/ass_3/weights/task_A_part_e/'+str(1000)\n",
    "    save_path=saver.save(sess,folder+'/accu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part F</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "import matplotlib.gridspec as gridspec # subplots\n",
    "from pylab import plot, ylim, xlim, show, xlabel, ylabel, grid\n",
    "import matplotlib\n",
    "from __future__ import division\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = .99\n",
    "num_episodes=2000\n",
    "num_eps_test=100\n",
    "e=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plh():\n",
    "    states=tf.placeholder(shape=[None,4],dtype=tf.float32) \n",
    "    nextQ = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
    "    return(states,nextQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph(states,nextQ):\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "        W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "        b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime=(tf.matmul(h2,W2)+b2)\n",
    "    maxQ1=tf.stop_gradient(tf.reduce_max(Q_prime,1))\n",
    "    loss = tf.reduce_mean(tf.square((nextQ) - Q))\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    return(Q,maxQ1,Q_prime,loss,trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "batchSize = 32\n",
    "bufferx=100\n",
    "h=0\n",
    "replay = []\n",
    "#stores tuples of (S, A, R, S')\n",
    "size=(101,1)\n",
    "jList = []\n",
    "lo_List = []\n",
    "avg_eps_len=[]\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "for epoch in range(num_episodes):\n",
    "    tf.reset_default_graph()\n",
    "    states,nextQ=plh()\n",
    "    Q,maxQ1,Q_prime,loss,trainer=graph(lr,states,nextQ)\n",
    "    saver = tf.train.Saver(write_version = tf.train.SaverDef.V2)\n",
    "    sess=tf.Session() \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            if (d==True):\n",
    "                r=-1    \n",
    "            else:\n",
    "                r=0\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            #Experience replay storage\n",
    "            if (len(replay) < bufferx): #if buffer not filled, add to it\n",
    "                replay.append((s, a, r, s1))\n",
    "            else: #if buffer full, overwrite old values\n",
    "                #randomly sample our experience replay memory\n",
    "                if (h < (bufferx-1)):\n",
    "                    h += 1\n",
    "                else:\n",
    "                    h = 0\n",
    "                replay[h] = (s, a, r, s1)        \n",
    "                observations=np.copy(replay)\n",
    "                batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batchSize)\n",
    "                batch_cur_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],0])),[batchSize,4])\n",
    "                batch_nex_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],3])),[batchSize,4])\n",
    "                actions=np.reshape(np.concatenate(observations[[batch_rows],1]),[batchSize,1])\n",
    "                rewards=np.reshape(np.concatenate(observations[[batch_rows+1],2]),[batchSize,1])\n",
    "                delta=rewards+1\n",
    "                current_Q=sess.run(Q,feed_dict={states:batch_cur_st})\n",
    "                target_Q=np.copy(current_Q)\n",
    "                max_Q1=sess.run(maxQ1,feed_dict={states:batch_nex_st})\n",
    "                max_Q1=np.reshape(max_Q1,[batchSize,1])\n",
    "                row_idx = np.array(range(batchSize))\n",
    "                target_Q[row_idx[:,None],[actions]]=delta*y*max_Q1+rewards \n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,nextQ:target_Q})\n",
    "                lo_List.append(lo)\n",
    "            if (d==True):\n",
    "                break\n",
    "            s=s1\n",
    "        if lo:\n",
    "            print(\"Episode {} with loss {}\".format(i,lo)) \n",
    "            \n",
    "        if ((i%2)==0):\n",
    "            eps_len=[]\n",
    "            for eps in range(num_eps_test):\n",
    "                st_te=env.reset()\n",
    "                st_te=np.reshape(st_te,[1,4])\n",
    "                t=0\n",
    "                while t < 300:\n",
    "                    t+=1\n",
    "                    test_Q=sess.run(Q_prime,feed_dict={states:st_te})\n",
    "                    act=np.argmax(test_Q)\n",
    "                    new_st_te,re,done,_ = env.step(act)\n",
    "                    new_st_te=np.reshape(new_st_te,[1,4])\n",
    "                    if (done==True):\n",
    "                        in_eps_len=np.copy(t)\n",
    "                        eps_len.append(in_eps_len)\n",
    "                        break\n",
    "                    else:\n",
    "                        st_te=new_st_te\n",
    "                if ((eps%99==0) & (eps>0)):\n",
    "                    print(\"Episode finished after {} timesteps\".format(t))  \n",
    "            avg_eps_len.append(np.mean(eps_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>part G</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states=tf.placeholder(shape=[None,4],dtype=tf.float32) \n",
    "with tf.name_scope(\"weights\"):\n",
    "    W1 = tf.get_variable(\"W1\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    W2 = tf.get_variable(\"W2\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b1 = tf.get_variable(\"b1\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "    b2 = tf.get_variable(\"b2\", [2],initializer=tf.constant_initializer(0,0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_ne=tf.placeholder(shape=[None,4],dtype=tf.float32) \n",
    "with tf.name_scope(\"weights\"):\n",
    "    W1_n = tf.get_variable(\"W1_n\", [4, 100],initializer=tf.random_normal_initializer(0,0.001))\n",
    "    W2_n = tf.get_variable(\"W2_n\", [100, 2],initializer=tf.random_normal_initializer(0,0.001))\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b1_n = tf.get_variable(\"b1_n\", [100],initializer=tf.constant_initializer(0,0.001))\n",
    "    b2_n = tf.get_variable(\"b2_n\", [2],initializer=tf.constant_initializer(0,0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred,Q,Q_prime=observation_to_action(states)\n",
    "loss = tf.reduce_mean(tf.square((nextQ) - Q))\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def observation_to_action(states):\n",
    "    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\n",
    "    Q=tf.matmul(h1, W2) + b2\n",
    "    h2 = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    predict=tf.argmax(Q,1)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def observation_to_action_n(s_ne):\n",
    "    h2_n = tf.nn.relu(tf.matmul(states,W1)+ b1)\n",
    "    Q_prime_n=tf.stop_gradient(tf.matmul(h2_n,W2_n)+b2_n)\n",
    "    predict=tf.argmax(Q_n,1)\n",
    "    return Q_prime_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "batchSize = 32\n",
    "bufferx=100\n",
    "h=0\n",
    "replay = []\n",
    "#stores tuples of (S, A, R, S')\n",
    "size=(101,1)\n",
    "jList = []\n",
    "lo_List = []\n",
    "avg_eps_len=[]\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "for run in range(1):\n",
    "    for i in range(num_episodes):\n",
    "        if (i%5==0):\n",
    "            copy weights 1 to weights 2 \n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s=np.reshape(s,[1,4])\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 300:\n",
    "            #env.render()\n",
    "            j+=1\n",
    "            allQ = sess.run(Q,feed_dict={states:s})\n",
    "            if np.random.rand(1) < e:\n",
    "                a = np.sum(np.random.uniform(0,1)>0.5)\n",
    "            else:\n",
    "                a=np.argmax(allQ)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            if (d==True):\n",
    "                r=-1    \n",
    "            else:\n",
    "                r=0\n",
    "            s1=np.reshape(s1,[1,4])\n",
    "            #Experience replay storage\n",
    "            if (len(replay) < bufferx): #if buffer not filled, add to it\n",
    "                replay.append((s, a, r, s1))\n",
    "            else: #if buffer full, overwrite old values\n",
    "                #randomly sample our experience replay memory\n",
    "                if (h < (bufferx-1)):\n",
    "                    h += 1\n",
    "                else:\n",
    "                    h = 0\n",
    "                replay[h] = (s, a, r, s1)        \n",
    "                observations=np.copy(replay)\n",
    "                batch_rows=np.random.randint(np.shape(observations)[0]-1, size=batchSize)\n",
    "                batch_cur_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],0])),[batchSize,4])\n",
    "                batch_nex_st=np.reshape(np.concatenate(np.concatenate(observations[[batch_rows],3])),[batchSize,4])\n",
    "                actions=np.reshape(np.concatenate(observations[[batch_rows],1]),[batchSize,1])\n",
    "                rewards=np.reshape(np.concatenate(observations[[batch_rows+1],2]),[batchSize,1])\n",
    "                delta=rewards+1\n",
    "                current_Q=sess.run(Q,feed_dict={states:batch_cur_st})\n",
    "                target_Q=np.copy(current_Q)\n",
    "                Q_s_prime=sess.run(Q_prime,feed_dict={s_ne:batch_nex_st})\n",
    "                maxQ1=np.reshape(np.max(Q_s_prime,1),[batchSize,1])\n",
    "                row_idx = np.array(range(batchSize))\n",
    "                target_Q[row_idx[:,None],[actions]]=delta*y*maxQ1+rewards \n",
    "                _,lo=sess.run([trainer,loss],feed_dict={states:batch_cur_st,nextQ:target_Q})\n",
    "                lo_List.append(lo)\n",
    "            if (d==True):\n",
    "                break\n",
    "            s=s1\n",
    "        if lo:\n",
    "            print(\"Episode {} with loss {}\".format(i,lo)) \n",
    "            \n",
    "        if ((i%2)==0):\n",
    "            eps_len=[]\n",
    "            for eps in range(num_eps_test):\n",
    "                st_te=env.reset()\n",
    "                st_te=np.reshape(st_te,[1,4])\n",
    "                t=0\n",
    "                while t < 300:\n",
    "                    t+=1\n",
    "                    test_Q=sess.run(Q_prime,feed_dict={states:st_te})\n",
    "                    act=np.argmax(test_Q)\n",
    "                    new_st_te,re,done,_ = env.step(act)\n",
    "                    new_st_te=np.reshape(new_st_te,[1,4])\n",
    "                    if (done==True):\n",
    "                        in_eps_len=np.copy(t)\n",
    "                        eps_len.append(in_eps_len)\n",
    "                        break\n",
    "                    else:\n",
    "                        st_te=new_st_te\n",
    "                if ((eps%99==0) & (eps>0)):\n",
    "                    print(\"Episode finished after {} timesteps\".format(t))  \n",
    "            avg_eps_len.append(np.mean(eps_len))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
